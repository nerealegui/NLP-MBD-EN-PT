{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nerealegui/NLP-MBD-EN-PT/blob/main/tagging_parsing_practice/bag_of_words.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LCvwbQnTvBRh"
      },
      "source": [
        "# Google Colab Configuration\n",
        "\n",
        "**Execute this steps to configure the Google Colab environment in order to execute this notebook. It is not required if you are executing it locally and you have properly configured your local environment according to what explained in the Github Repository.**\n",
        "\n",
        "The first step is to clone the repository to have access to all the data and files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "gakj_n0h70ox"
      },
      "outputs": [],
      "source": [
        "repository_name = \"NLP-MBD-EN-PT\"\n",
        "repository_url = 'https://github.com/nerealegui/' + repository_name"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3d7mC64KvlwP",
        "outputId": "d39140c3-9726-4ca5-d2d6-228a2f402a5e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "fatal: destination path 'NLP-MBD-EN-PT' already exists and is not an empty directory.\n"
          ]
        }
      ],
      "source": [
        "! git clone $repository_url"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ecfec2Y4v6e9"
      },
      "source": [
        "Install the requirements"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JHDzMQwpyODo"
      },
      "source": [
        "Now you have everything you need to execute the code in Colab"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HeGC-Qg3sK8Y"
      },
      "source": [
        "# Bag-of-words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r_8zOo4gsK8f",
        "outputId": "90009fdd-fcde-4478-f7b2-9f155b7bc2fd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Requirement already satisfied: nltk in /Users/nerealegui/Library/Python/3.9/lib/python/site-packages (3.9.1)\n",
            "Requirement already satisfied: tqdm in /Users/nerealegui/Library/Python/3.9/lib/python/site-packages (from nltk) (4.67.1)\n",
            "Requirement already satisfied: joblib in /Users/nerealegui/Library/Python/3.9/lib/python/site-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: click in /Users/nerealegui/Library/Python/3.9/lib/python/site-packages (from nltk) (8.1.8)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /Users/nerealegui/Library/Python/3.9/lib/python/site-packages (from nltk) (2024.11.6)\n",
            "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 25.0.1 is available.\n",
            "You should consider upgrading via the '/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Requirement already satisfied: pandas in /Users/nerealegui/Library/Python/3.9/lib/python/site-packages (2.2.3)\n",
            "Requirement already satisfied: pytz>=2020.1 in /Users/nerealegui/Library/Python/3.9/lib/python/site-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/nerealegui/Library/Python/3.9/lib/python/site-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /Users/nerealegui/Library/Python/3.9/lib/python/site-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: numpy>=1.22.4 in /Users/nerealegui/Library/Python/3.9/lib/python/site-packages (from pandas) (2.0.2)\n",
            "Requirement already satisfied: six>=1.5 in /Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/site-packages (from python-dateutil>=2.8.2->pandas) (1.15.0)\n",
            "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 25.0.1 is available.\n",
            "You should consider upgrading via the '/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Requirement already satisfied: numpy in /Users/nerealegui/Library/Python/3.9/lib/python/site-packages (2.0.2)\n",
            "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 25.0.1 is available.\n",
            "You should consider upgrading via the '/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "# Install missing packages\n",
        "%pip install nltk\n",
        "%pip install pandas\n",
        "%pip install numpy\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package abc to /Users/nerealegui/nltk_data...\n",
            "[nltk_data]   Package abc is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     /Users/nerealegui/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# NLTK Corpora  in https://www.nltk.org/nltk_data/\n",
        "import nltk\n",
        "nltk.download('abc')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "import pandas as pd\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "onkwLfBQsK8g"
      },
      "source": [
        "The `nltk` library includes several corpus for experimentation. In this markdown we are going to make use of the corpus including the set of abc's plays.\n",
        "\n",
        "In the following cell, I will load the corpus and create a dataframe with the name of the book and the textual content."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pFhGK1uasK8h",
        "outputId": "3992a3af-baac-4b4f-a352-35f7c2d35b97"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['rural.txt', 'science.txt']\n",
            "          book                                              words\n",
            "0    rural.txt  PM denies knowledge of AWB kickbacks The Prime...\n",
            "1  science.txt  Cystic fibrosis affects 30 , 000 children and ...\n"
          ]
        }
      ],
      "source": [
        "abc_df = pd.DataFrame(columns=[\"book\", \"words\"])\n",
        "print(nltk.corpus.abc.fileids())\n",
        "for ii, book in enumerate(nltk.corpus.abc.fileids()):\n",
        "    abc_df.loc[ii] = (book, \" \".join(nltk.corpus.abc.words(book)))\n",
        "print(abc_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0xgnNtVnsK8i"
      },
      "source": [
        "While this representation can be useful for humans, it is of no use if you want to use these data for an NLP system.\n",
        "\n",
        "As we discussed in class, we need to create the document-term matrix which will be the input for any NLP system we need to create on top of it. In the document term matrix we have a row for each one of the different documents (the abc's plays) and a column for each one of the words in the dataset. At each cell, you will find the weight of the word in the document (for example, how many times does the word appear in the document).\n",
        "\n",
        "In class we presented several weighting approaches, let's see how we can create them."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XdJiO9XvsK8i"
      },
      "source": [
        "Let's start with the simplest one: The Binary weighting. Binary weighting only defines if a word appears (1) or does not appear (0) in a document"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TdxurLNi9clF"
      },
      "source": [
        "Here’s what happens in that snippet, step by step:\n",
        "\n",
        "1.  Instantiate the vectorizer  \n",
        "     `binary_weighting = CountVectorizer(binary=True)`  \n",
        "      – Builds a vocabulary and, when transforming, will mark each word as present (1) or absent (0) in a document.\n",
        "\n",
        "2.  Learn the vocabulary and transform the text into a sparse matrix  \n",
        "     `binary_abc = binary_weighting.fit_transform(abc_df.words)`  \n",
        "      – `fit` scans all documents to build the vocab (one column per unique token).  \n",
        "      – `transform` turns each document into a row of 0/1s, stored in a SciPy sparse matrix of shape `(n_docs, n_terms)`.\n",
        "\n",
        "3.  Convert to a pandas DataFrame  \n",
        "     `binary_abc.toarray()`  \n",
        "      – Converts the sparse matrix to a dense NumPy array.  \n",
        "     `columns=binary_weighting.get_feature_names_out()`  \n",
        "      – Labels each column with the corresponding token.  \n",
        "     `pd.DataFrame(...)`  \n",
        "      – Wraps the array in a DataFrame for easy inspection.\n",
        "\n",
        "4.  Inspect the result  \n",
        "     `print(binary_dt_matrix)`  \n",
        "      – Shows a table where each row is a play, each column is a word, and each cell is 0 or 1 indicating absence/presence.\n",
        "\n",
        "This binary‐weighted document–term matrix is useful when you only care about whether a term occurs at all, not how often."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jg_NgZWysK8j",
        "outputId": "99030932-224d-4015-8e93-55b4dbf34e4b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   00  000  000ºc  00am  00pm  010  02  02811  03  03549  ...  zoology  zoom  \\\n",
            "0   1    1      0     1     1    1   1      0   1      0  ...        0     0   \n",
            "1   0    1      1     0     0    0   1      1   1      1  ...        1     1   \n",
            "\n",
            "   zooming  zooplankton  zoos  zuberb  zucchini  zukerman  zulu  zwingmann  \n",
            "0        0            0     0       0         0         0     1          0  \n",
            "1        1            1     1       1         1         1     0          1  \n",
            "\n",
            "[2 rows x 27623 columns]\n"
          ]
        }
      ],
      "source": [
        "# Convert a collection of text documents to a matrix of token counts. This implementation produces a sparse representation of the counts using scipy.sparse.csr_matrix.\n",
        "binary_weighting = CountVectorizer(binary=True)\n",
        "binary_abc = binary_weighting.fit_transform(abc_df.words)\n",
        "binary_dt_matrix = pd.DataFrame(binary_abc.toarray(), columns=binary_weighting.get_feature_names_out())\n",
        "print(binary_dt_matrix)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KmEaF25isK8j"
      },
      "source": [
        "Let's inspect the most and least important terms related to the document 6 (Othello)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VDXuF1qasK8k",
        "outputId": "7c873da5-9717-483e-cc6b-52c7b6d4c782"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "25 most important terms for document science.txt\n",
            "zwingmann          1\n",
            "fun                1\n",
            "fulfilling         1\n",
            "full               1\n",
            "fuller             1\n",
            "fullerenes         1\n",
            "fully              1\n",
            "fumed              1\n",
            "fumes              1\n",
            "fumigating         1\n",
            "fumo               1\n",
            "function           1\n",
            "fujiwara           1\n",
            "functional         1\n",
            "functionalised     1\n",
            "functionality      1\n",
            "functioning        1\n",
            "functions          1\n",
            "fund               1\n",
            "fundaci            1\n",
            "fundamental        1\n",
            "fundamentalists    1\n",
            "fuk                1\n",
            "fujinaga           1\n",
            "gainesville        1\n",
            "Name: 1, dtype: int64\n",
            "25 least important terms for document science.txt\n",
            "earmarked       0\n",
            "earn            0\n",
            "cellar          0\n",
            "tabulate        0\n",
            "productively    0\n",
            "integral        0\n",
            "cdma            0\n",
            "cease           0\n",
            "ceased          0\n",
            "tablelands      0\n",
            "cecil           0\n",
            "tableland       0\n",
            "ceduna          0\n",
            "tablegrapes     0\n",
            "celcius         0\n",
            "earthier        0\n",
            "celebrates      0\n",
            "celebrating     0\n",
            "germinated      0\n",
            "earns           0\n",
            "earnings        0\n",
            "earners         0\n",
            "oster           0\n",
            "earner          0\n",
            "00              0\n",
            "Name: 1, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "document = 1\n",
        "print(\"25 most important terms for document\", abc_df.iloc[document]['book'])\n",
        "print(binary_dt_matrix.iloc[:, np.argsort(binary_dt_matrix.loc[document])[::-1]].iloc[document][:25])\n",
        "\n",
        "print(\"25 least important terms for document\", abc_df.iloc[document]['book'])\n",
        "print(binary_dt_matrix.iloc[:, np.argsort(binary_dt_matrix.loc[document])[::-1]].iloc[document][-25:])\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rKzkc_KysK8k"
      },
      "source": [
        "As you can see, the representation is not very useful as it is. By only telling us if a word appears or not in a document is not giving us a lot of information. **Can you think on a situation where this binary weighting can be sufficient?**\n",
        "\n",
        "The next thing to know will be whether the word appears only once or several times."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J7N3XRgdsK8l",
        "outputId": "22040afd-92e1-492a-ef2c-9ff7a56150fe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   00  000  000ºc  00am  00pm  010  02  02811  03  03549  ...  zoology  zoom  \\\n",
            "0  13  451      0     3     2    1   1      0   1      0  ...        0     0   \n",
            "1   0  248      1     0     0    0   1      1   1      1  ...        4     1   \n",
            "\n",
            "   zooming  zooplankton  zoos  zuberb  zucchini  zukerman  zulu  zwingmann  \n",
            "0        0            0     0       0         0         0     1          0  \n",
            "1        2            7     2       2         1         1     0          2  \n",
            "\n",
            "[2 rows x 27623 columns]\n"
          ]
        }
      ],
      "source": [
        "tf_weighting = CountVectorizer() # now the vector is not binary\n",
        "tf_abc = tf_weighting.fit_transform(abc_df.words)\n",
        "tf_dt_matrix = pd.DataFrame(tf_abc.toarray(), columns=tf_weighting.get_feature_names_out())\n",
        "print(tf_dt_matrix)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q0L566RDsK8m"
      },
      "source": [
        "Ok, now we have the words weighted according to how many times they appear in the document.\n",
        "\n",
        "Let's check now the most and least important words in Othello"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h1NAk703sK8m",
        "outputId": "e70749b9-74d7-41a9-ee5d-b338dcca480b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "25 most important terms for document science.txt\n",
            "the     23705\n",
            "of      11878\n",
            "to       9068\n",
            "and      8665\n",
            "in       7549\n",
            "that     4960\n",
            "says     4474\n",
            "is       4016\n",
            "it       2997\n",
            "for      2723\n",
            "are      2357\n",
            "be       2286\n",
            "as       2198\n",
            "on       2173\n",
            "with     2135\n",
            "have     2130\n",
            "they     2063\n",
            "from     2028\n",
            "at       1974\n",
            "by       1760\n",
            "he       1703\n",
            "this     1669\n",
            "but      1665\n",
            "or       1501\n",
            "an       1435\n",
            "Name: 1, dtype: int64\n",
            "25 least important terms for document science.txt\n",
            "duel        0\n",
            "duffing     0\n",
            "duffy       0\n",
            "dugdale     0\n",
            "saddam      0\n",
            "dunkeld     0\n",
            "dunmall     0\n",
            "dunmore     0\n",
            "dunn        0\n",
            "dunsmore    0\n",
            "dunstan     0\n",
            "duping      0\n",
            "sackings    0\n",
            "duress      0\n",
            "durham      0\n",
            "sack        0\n",
            "durong      0\n",
            "durum       0\n",
            "dusting     0\n",
            "sabotage    0\n",
            "sabien      0\n",
            "dutton      0\n",
            "sabah       0\n",
            "rutile      0\n",
            "00          0\n",
            "Name: 1, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "document = 1\n",
        "print(\"25 most important terms for document\", abc_df.iloc[document]['book'])\n",
        "print(tf_dt_matrix.iloc[:, np.argsort(tf_dt_matrix.loc[document])[::-1]].iloc[document][:25])\n",
        "\n",
        "print(\"25 least important terms for document\", abc_df.iloc[document]['book'])\n",
        "print(tf_dt_matrix.iloc[:, np.argsort(tf_dt_matrix.loc[document])[::-1]].iloc[document][-25:])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HFSHiEEGsK8n"
      },
      "source": [
        "**What problem do you see with the most important words? Are they really representative?**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8CiGiYtasK8n"
      },
      "source": [
        "Let's check now how to create the TF-IDF weighting to see if we can improve this representation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8vWjuucF-umm"
      },
      "source": [
        "**TF-IDF (Term Frequency-Inverse Document Frequency)** features are numerical representations of text that reflect the importance of a word within a document relative to a collection of documents\n",
        "\n",
        "- **Term Frequency (TF):**\n",
        "Measures how often a word appears in a specific document. A higher TF indicates the word is more important within that document.\n",
        "\n",
        "- **Inverse Document Frequency (IDF):**\n",
        "Measures how rare or common a word is across the entire corpus of documents. Rare words have a higher IDF, indicating they are more informative and important."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bAfISl2jsK8o",
        "outputId": "06d57b34-786c-4c54-d0da-d1e97664e7cf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "         00       000     000ºc     00am      00pm       010        02  \\\n",
            "0  0.000692  0.017086  0.000000  0.00016  0.000106  0.000053  0.000038   \n",
            "1  0.000000  0.007449  0.000042  0.00000  0.000000  0.000000  0.000030   \n",
            "\n",
            "      02811        03     03549  ...   zoology      zoom   zooming  \\\n",
            "0  0.000000  0.000038  0.000000  ...  0.000000  0.000000  0.000000   \n",
            "1  0.000042  0.000030  0.000042  ...  0.000169  0.000042  0.000084   \n",
            "\n",
            "   zooplankton      zoos    zuberb  zucchini  zukerman      zulu  zwingmann  \n",
            "0     0.000000  0.000000  0.000000  0.000000  0.000000  0.000053   0.000000  \n",
            "1     0.000295  0.000084  0.000084  0.000042  0.000042  0.000000   0.000084  \n",
            "\n",
            "[2 rows x 27623 columns]\n"
          ]
        }
      ],
      "source": [
        "tf_idf_weighting = TfidfVectorizer()\n",
        "tf_idf_abc = tf_idf_weighting.fit_transform(abc_df.words)\n",
        "tf_idf_dt_matrix = pd.DataFrame(tf_idf_abc.toarray(), columns=tf_idf_weighting.get_feature_names_out())\n",
        "print(tf_idf_dt_matrix)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GEwT8UxAsK8o",
        "outputId": "9384f489-60fc-4735-b3ad-1b09b454a398"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "25 most important terms for document science.txt\n",
            "the     0.711989\n",
            "of      0.356761\n",
            "to      0.272361\n",
            "and     0.260257\n",
            "in      0.226737\n",
            "that    0.148976\n",
            "says    0.134378\n",
            "is      0.120622\n",
            "it      0.090016\n",
            "for     0.081786\n",
            "are     0.070793\n",
            "be      0.068661\n",
            "as      0.066018\n",
            "on      0.065267\n",
            "with    0.064126\n",
            "have    0.063975\n",
            "they    0.061963\n",
            "from    0.060912\n",
            "at      0.059290\n",
            "by      0.052862\n",
            "he      0.051150\n",
            "this    0.050129\n",
            "but     0.050009\n",
            "or      0.045083\n",
            "an      0.043101\n",
            "Name: 1, dtype: float64\n",
            "25 least important terms for document science.txt\n",
            "ecumenical        0.0\n",
            "schuller          0.0\n",
            "eddie             0.0\n",
            "eden              0.0\n",
            "schneider         0.0\n",
            "edmonds           0.0\n",
            "edmund            0.0\n",
            "edna              0.0\n",
            "schild            0.0\n",
            "eion              0.0\n",
            "schemes           0.0\n",
            "educating         0.0\n",
            "schembri          0.0\n",
            "edwards           0.0\n",
            "scheduling        0.0\n",
            "schaap            0.0\n",
            "sceptics          0.0\n",
            "scenic            0.0\n",
            "egan              0.0\n",
            "scaremongering    0.0\n",
            "egrets            0.0\n",
            "eighties          0.0\n",
            "eighty            0.0\n",
            "scandals          0.0\n",
            "00                0.0\n",
            "Name: 1, dtype: float64\n"
          ]
        }
      ],
      "source": [
        "document = 1\n",
        "print(\"25 most important terms for document\", abc_df.iloc[document]['book'])\n",
        "print(tf_idf_dt_matrix.iloc[:, np.argsort(tf_idf_dt_matrix.loc[document])[::-1]].iloc[document][:25])\n",
        "\n",
        "print(\"25 least important terms for document\", abc_df.iloc[document]['book'])\n",
        "print(tf_idf_dt_matrix.iloc[:, np.argsort(tf_idf_dt_matrix.loc[document])[::-1]].iloc[document][-25:])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HxSFxfWqsK8p"
      },
      "source": [
        "**What do you see now in the representation? Have we solved all the problems?**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZL1a-Y01sK8p"
      },
      "source": [
        "# StopWords\n",
        "\n",
        "In the previous section we have experimenting some problems related to stopwords, such as `and` or `of`. These words do not carry any meaning and are unlikely to provide any advantage for any subsequent NLP task and, therefore, we are safe to remove them.\n",
        "\n",
        "Let's see how to do it via NLTK.\n",
        "\n",
        "Since stopwords are language-dependant, NLTK provides a list for several languages."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fVxZRw4dsK8p",
        "outputId": "5abd70d2-6119-4c38-9c6e-85e9e5ed714a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Languages for which NLTK provides an stopword list: albanian, arabic, azerbaijani, basque, belarusian, bengali, catalan, chinese, danish, dutch, english, finnish, french, german, greek, hebrew, hinglish, hungarian, indonesian, italian, kazakh, nepali, norwegian, portuguese, romanian, russian, slovene, spanish, swedish, tajik, tamil, turkish\n"
          ]
        }
      ],
      "source": [
        "from nltk.corpus import stopwords\n",
        "print(\"Languages for which NLTK provides an stopword list:\", \", \".join(stopwords.fileids()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bKl8SwQ8sK8q"
      },
      "source": [
        "We are just interested in the English stopword list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qUeIICa1sK8q",
        "outputId": "8ed92023-c89d-4bc4-cffc-ed85a3f101eb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Example of 25 English stopwords: a, about, above, after, again, against, ain, all, am, an, and, any, are, aren, aren't, as, at, be, because, been, before, being, below, between, both\n"
          ]
        }
      ],
      "source": [
        "print(\"Example of 25 English stopwords:\", \", \".join(stopwords.words(\"english\")[:25]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9zpkp2lisK8r"
      },
      "source": [
        "We can use this list to remove these words from our representation and create the document term matrix without them. Let's check."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a5XKr4U4sK8r",
        "outputId": "38dc083c-2aad-4245-f776-9ab7b8e3627b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "         00       000     000ºc      00am      00pm       010        02  \\\n",
            "0  0.002867  0.070776  0.000000  0.000662  0.000441  0.000221  0.000157   \n",
            "1  0.000000  0.038415  0.000218  0.000000  0.000000  0.000000  0.000155   \n",
            "\n",
            "      02811        03     03549  ...   zoology      zoom   zooming  \\\n",
            "0  0.000000  0.000157  0.000000  ...  0.000000  0.000000  0.000000   \n",
            "1  0.000218  0.000155  0.000218  ...  0.000871  0.000218  0.000435   \n",
            "\n",
            "   zooplankton      zoos    zuberb  zucchini  zukerman      zulu  zwingmann  \n",
            "0     0.000000  0.000000  0.000000  0.000000  0.000000  0.000221   0.000000  \n",
            "1     0.001524  0.000435  0.000435  0.000218  0.000218  0.000000   0.000435  \n",
            "\n",
            "[2 rows x 27329 columns]\n"
          ]
        }
      ],
      "source": [
        "sw_free_tf_idf_weighting = TfidfVectorizer(stop_words='english')\n",
        "sw_free_tf_idf_abc = sw_free_tf_idf_weighting.fit_transform(abc_df.words)\n",
        "sw_free_tf_idf_dt_matrix = pd.DataFrame(sw_free_tf_idf_abc.toarray(), columns=sw_free_tf_idf_weighting.get_feature_names_out())\n",
        "print(sw_free_tf_idf_dt_matrix)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "97woQ18ssK8r",
        "outputId": "01909e19-b437-481c-9e0e-26276fca4ff5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "25 most important terms for document science.txt\n",
            "says           0.693014\n",
            "new            0.158306\n",
            "researchers    0.157376\n",
            "research       0.139408\n",
            "say            0.137704\n",
            "university     0.135691\n",
            "people         0.123764\n",
            "study          0.120201\n",
            "years          0.114625\n",
            "scientists     0.113540\n",
            "like           0.111527\n",
            "dr             0.097586\n",
            "professor      0.087363\n",
            "used           0.081012\n",
            "team           0.078378\n",
            "time           0.074506\n",
            "australian     0.070014\n",
            "australia      0.068310\n",
            "human          0.063198\n",
            "journal        0.062424\n",
            "technology     0.059326\n",
            "cells          0.058551\n",
            "year           0.057312\n",
            "make           0.056383\n",
            "world          0.056228\n",
            "Name: 1, dtype: float64\n",
            "25 least important terms for document science.txt\n",
            "reconciliation     0.0\n",
            "gartrell           0.0\n",
            "cbh                0.0\n",
            "garth              0.0\n",
            "reclassified       0.0\n",
            "garside            0.0\n",
            "thiele             0.0\n",
            "thiel              0.0\n",
            "thieblemont        0.0\n",
            "garrett            0.0\n",
            "garrard            0.0\n",
            "garnishes          0.0\n",
            "reclassify         0.0\n",
            "thevissen          0.0\n",
            "garland            0.0\n",
            "thevenard          0.0\n",
            "gardner            0.0\n",
            "caveats            0.0\n",
            "gardiner           0.0\n",
            "gardeners          0.0\n",
            "recommit           0.0\n",
            "ganglioneuritis    0.0\n",
            "cazaly             0.0\n",
            "cbd                0.0\n",
            "00                 0.0\n",
            "Name: 1, dtype: float64\n"
          ]
        }
      ],
      "source": [
        "document = 1\n",
        "print(\"25 most important terms for document\", abc_df.iloc[document]['book'])\n",
        "print(sw_free_tf_idf_dt_matrix.iloc[:, np.argsort(sw_free_tf_idf_dt_matrix.loc[document])[::-1]].iloc[document][:25])\n",
        "\n",
        "print(\"25 least important terms for document\", abc_df.iloc[document]['book'])\n",
        "print(sw_free_tf_idf_dt_matrix.iloc[:, np.argsort(sw_free_tf_idf_dt_matrix.loc[document])[::-1]].iloc[document][-25:])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F-lBaRWcsK8s"
      },
      "source": [
        "It's much better now, isn't it?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ou53w9tCsK8s"
      },
      "source": [
        "Try to play with the previous code, change the document to see how the different weightings affect their representation or to use a different corpus from the ones included in NLTK"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "include_colab_link": true,
      "name": "bag_of_words.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
